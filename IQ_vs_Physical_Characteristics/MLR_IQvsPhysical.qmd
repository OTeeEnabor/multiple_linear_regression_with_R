---
title: "Untitled"
format: html
editor: visual
---

# MLR: IQ vs. Physical Characteristics

## Import Libraries

```{r}
# load necessary libraries
library(ggplot2)
library(GGally)
library(dplyr)
library(tidyverse)
library(janitor)
library(here)
library(gt)
library(gtsummary)
library(knitr)
library(DT)
library(maps)
library(sf)
library(leaflet)
```

## Background

What physical characteristics of human can we use to determine their intelligence? This question has been fundamental to the study of human intelligence. A group of research who set out to answer the question "***Are a person's brain size and body size predictive of his or her intelligence"?*** collected a sample of 38 college students.

The researchers collected the following variables for each student in their sample:

-   Response (*y*): performance IQ scores (PIQ) from the revised Wechsler Adult Intelligence Scale. The researchers used this as the measure of a person's intelligence.

-   **Potential predictor** (x1): Size of the brain based on the count obtained from MRI scans (measured as count per 10 000)

-   Potential predictor (x2): Height in inches

-   Potential predictor (x3): Weight in pounds

## Load data set

```{r}
# read in the dataset
df <-  read_table(here::here("IQ_vs_Physical_Characteristics/data/raw/index.txt"))
# show first 6 observations
head(df)
```

## Exploratory Data Analysis

```{r}
#| label: fig-scatter-matrix1
#| fig-cap: Scatter-plot matrix (base) showing the pair-wise relationship between the variables of the dataset.
# Scatter plot matrix in base R
pairs(~PIQ + Brain + Height + Weight, data = df, lower.panel = panel.smooth)
```

```{r}
#| label: fig-scatter-matrix2
#| fig-cap: Scatter-plot matrix showing the pair-wise relationship between the variables of the dataset.
df |> ggpairs(
  columns = c("PIQ", "Brain", "Weight", "Height"),
  upper = list(continuous = wrap("cor", size= 8))
) 
```

The scatter plot is simply for data checking - any errors in the data? The scatter plots also show the **marginal relationships** between each variable pair without including the other variables. From Figure @fig-scatter-matrix2, we can see that the **brain size** could be a strong candidate for a predictor of intelligence based on the high correlation value $0.378$. However, since this is a multiple linear regression problem, we would like to see how the response variable is influenced by all three predictors simultaneously.

Regression analysis begins by formulating a model to represent our data set. The regression model for this case is stated below:

$$
y_i = (\beta_0 +\beta1x_{i1}+\beta2x_{i2}+\beta3x_{i3})+\epsilon_i
$$

Where:

-   $y_i \text{ is the intelligence score (PIQ) of student}_i$

-   $x_i1 \text{ is the brain size (MRI) of student}_i$

-   \$xi2 \text{ is the height (Height) of student}\_i\$

-   \$ x\_{i3} \text{ is the weight (Weight) of student}\_i \$

-   \$ \epsilon\_i \text{ is the independent error term that is normally distributed about 0 with equal variance } \sigma\^2 \$

|     |
|-----|
|     |

The purpose of doing a regression analysis is usually to answer a question about our data set. The following are possible research questions:

1.  Which predictors can explain the variation in the IQ scores of the students? (**conduct hypothesis tests for individually testing whether each slope could be 0.**)
2.  What is the effect of brain size on student IQ scores, after considering height and weight? (**Calculate and interpret a confidence interval for the brain size slope parameter)**
3.  What is the PIQ of an individual with a given brain size, height, and weight? **(Calculate and interpret a prediction interval for the response)**

## Regression Model

```{r}
# create linear regression model
df.regression <- lm(PIQ ~ Brain+Height+Weight, data=df)
summary(df.regression)
```

From the summary information above, the regression equation for this model is as follows:

$PIQ = 111.4 + 2.060Brain \; - 2.73 Weight \; + 0.0006Height$

The summary information tells us the following:

-   The $R^2$ value of 0.2949 means that 29.49% of the variation in the response variable (PIQ) can be explained by the predictor variables - Brain, Weight, and Height

-   The p-value for the F-statistic (p=0.0072) indicates that the model with the chosen independent variables would perform better than a model that does not take into account these independent variables.

-   The p-values for the slope coefficients of Brain (P=0.000856) and Height (P=0.033) indicate that we can reject the null hypothesis that there is no linear relationship between these two variables and the response variable (PIQ). However, for the p-value for Weight (p=0.998) indicates that we cannot reject the null hypothesis.

# MLR: Underground Air Quality

## Background

Researchers (Colby, et al, 1987) set out to answer the research question - ***what are the breathing habits of baby birds that live in underground burrows?*** The researchers did a randomized experiment of n=120 nestling bank swallows. The experiment was conducted in an underground burrow, where the researchers varied the percentage of oxygen at 4 different levels (13%, 15%, 17%, and 19%) and the percentage of dioxide at 5 different levels (0%, 3%, 4.5%, 6%, and 9%). The resulting 20 experimental conditions, the researchers observed the total volume of air breathed per minute for each of 6 nestling bank swallows.

## Load data set

```{r}
df2 = read_table(here::here("Underground_Air_Quality/data/raw/index.txt"))
# show first 6 observations
head(df2)
```

The data set collected from the experiment is as follows:

-   **Response(y)**: percentage increase in "minute ventilation" (**Vent**), i.e, total volume of air breathed per minute.

-   **potential predictor (x1):** percentage of oxygen (O2) in the air baby birds are breathing

-   **potential predictor (x3):** percentage of carbon dioxide (CO2) in the air baby birds breathe

## Exploratory Data Analysis

```{r}
#| label: fig-scatter-matrix3
#| fig-cap: Scatter-plot matrix showing the pair-wise relationship between the variables of the dataset.
df2 |> ggpairs(
  columns = c("Vent", "O2", "CO2"),
  upper = list(continuous = wrap("cor", size= 8))
) 
```

Figure @fig-scatter-matrix3 above informs us of the following:

-   There is a strong correlation between the response variable (Vent) and predictor variable (CO2)

-   There is a weak correlation between the response variable (Vent) and predictor variable (O2)

-   There is no correlation between the chosen conditions of the experiment - CO2 and O2

There is another plot that is widely used in MLR scenarios where there is 1 response variable and 2 predictor variables, the **bubble plot**

```{r}
ggplot(df2,
       aes(
         x=`Vent`,
         y=`O2`,
         size=`CO2`
       )) +
  geom_point(alpha=0.75)+
  scale_size(range=c(.1,10), name = "CO2 levels")

```

## Regression Model

A regression model for this problem is stated below

\$\$

\$\$

$$
y_{i} = \beta_{0} +\beta_{1}x_{i1} +\beta_{2}x_{i2} + \epsilon_{i}
$$

Where:

-   $y_i \text{ percentage of minute ventilation of nestling bank swallow}_i$

-   $x_{i1} \text{ percentage of oxygen exposed to nestling bank swallow}_i$

-   \$ x\_{i2} \text{ percentage of Carbon dioxide exposed to nestling bank swallow}\_i \$

-   \$ \epsilon\_i \text{ is the independent error term that is normally distributed about 0 with equal variance } \sigma\^2 \$

**Research Question**

-   Is oxygen related to minute ventilation, after taking into account carbon dioxide? (Conduct a hypothesis test for testing whether the O2 slope parameter is 0.)

-   Is carbon dioxide related to minute ventilation, after taking into account oxygen? (Conduct a hypothesis test for testing whether the CO2 slope parameter is 0.)

-   What is the mean minute ventilation of all nestling bank swallows whose breathing air is comprised of 15% oxygen and 5% carbon dioxide? (Calculate and interpret a confidence interval for the mean response.)

```{r}
# create linear regression model
df2.regression <- lm(Vent ~ O2+CO2, data=df2)
summary(df2.regression)
```

## General Linear F-Test

This test is a three step approach:

1.  Define a larger **full model** (larger - a model with more parameters)
2.  Define a smaller **reduced model** (smaller - a model with fewer parameters)
3.  Use an **f-statistic** to decide on whether or not to reject the smaller reduced model in favour of the larger model.

### Full Model

This is sometimes referred to as the **unrestricted model,** and is considered to be the most appropriate for available data. In simple linear regression, the full model:

$$
y_{i} = (\beta_{0} + \beta_{1}x_{i1}) + \epsilon_{i}
$$

### Reduced model

This is referred to as the **restricted model** and is the model that is described by the null hypothesis $H_{0}$. In SLR, the common null hypothesis is $H_{0}:\beta_{1}=0$, i.e., there is no relationship between the response and predictor variable. The reduced model is:

$$y_{i} = \beta_{0}+\epsilon_{i}$$

### Test

A test is used to determine which model between the reduced model and the full model performs better in describing the data trend when it cannot be determined by a plot. This test involves quantifying the error that remains after fitting each of the two models to the data. This involves performing the **general linear F-test approach:**

1.  **"fit dull model"** to the data
    1.  Calculate the least square estimates of $\beta_{0} \text{ and } \beta_{1}$.
    2.  Calculate the error sum of squares, which is denoted as "SSE(F)"
2.  "**fit the reduced model"** to the data
    1.  obtain the least squares estimate of $\beta_{0}$
    2.  Calculate the error sum of squares, which is denoted as "SSE(R)"

### Examples

```{r}
df3 <- read_table(here::here("IQ_vs_Physical_Characteristics/data/raw/heightgpdata.txt"))
# show first few observations
head(df3)
```

```{r}
head(df3)
```

```{r}
#| label: fig-scatter-matrix4
#| fig-cap: Scatter-plot matrix showing the pair-wise relationship between the variables of the dataset.
df3 |> ggpairs(
  columns = c("height","gpa" ),
  upper = list(continuous = wrap("cor", size= 8))
) 
```

Lets now plot the 3rd graph with two estimated regression lines - full model and reduced model.

```{r}
# create linear regression model
lm_fit <- lm(gpa~height, data=df3)
summary(lm_fit)

```

```{r}
# create a new data frame that contains the independent variable - height, true value and predicted values of the dependent variable
df3_predict = data.frame(
  height = df3$height,
  gpa_true = df3$gpa,
  gpa_pred_full = predict(lm_fit, df3),
  gpa_pred_reduced = mean(df3$gpa)
)
# show the first few observations of the new dataframe
head(df3_predict)
```

```{r}

ggplot(
  df3, aes(
    x=height,
    y=gpa)) +
  geom_point()+
  geom_line(
    color="black",
    data=df3_predict,
    aes(
      x=height,
      y = gpa_pred_full
    )
  )+
  geom_line(
    linetype = "dashed",
    color="black",
    data=df3_predict,
    aes(
      x=height,
      y = gpa_pred_reduced
    )
  )
```

The estimated lines are quite similar and now to calculate the error sum of squares for each model.

```{r}
df3_predict$SSE_full <- (df3_predict$gpa_true-df3_predict$gpa_pred_full)^2
df3_predict$SSE_reduced <- (df3_predict$gpa_true-df3_predict$gpa_pred_reduced)^2
sum(df3_predict$SSE_full)
sum(df3_predict$SSE_reduced)
```

The two error sum of squares for each model are different by 0.0276, this means that there is little change created in the prediction of GPA when the `height` variable is included in the model. The following can be used to summarize why we use the general linear F-test approach:

-   The general linear F-test involves a comparison between SSE(R) and SSE(F).

-   SSE(R) can never be smaller than SSE(F). It is always larger than (or possibly the same as) SSE(F).

    -   If the SSE(F) and the SSE(R) are close, then the variation around the estimated full model regression function is almost as large as the variation around the estimated reduced model regression function. In that case, the simpler reduced model should be used.

    -   If the SSE(F) and SSE(R) are far apart, then the additional parameters reduce the variation around the estimated regression function, then it is advised to use the larger full model.

The **general linear F-statistic** is used to justify how different the SSE(R) has to be from the SSE(F)

$$
F^{*} =  (\frac{SSE(R) -SSE(F)}{df_{R} - df_{F}}) \div (\frac{SSE(F)}{df_{F}})
$$

The **general linear F-statistic** is used to decide whether or not to reject the null hypothesis $H_{0}$: the reduce model in favour of the alternative hypothesis $H_{a}$: the full model.

**In general,** the $H_{O}$ is rejected if $F^{*}$ is large - or equivalently if its associated P-value is small.
